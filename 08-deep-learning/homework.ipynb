{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# !wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
        "# !unzip data.zip"
      ],
      "metadata": {
        "id": "JABVNNop71xT"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "In this homework, we'll build a model for classifying various hair types.\n",
        "For this, we will use the Hair Type dataset that was obtained from\n",
        "[Kaggle](https://www.kaggle.com/datasets/kavyasreeb/hair-type-dataset)\n",
        "and slightly rebuilt.\n",
        "\n",
        "You can download the target dataset for this homework from\n",
        "[here](https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip):\n",
        "\n",
        "```bash\n",
        "wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
        "unzip data.zip\n",
        "```\n",
        "The dataset is split into train and test dataset. Use train dataset to train the model and test dataset for validation.\n",
        "\n",
        "In the lectures we saw how to use a pre-trained neural network. In the homework, we'll train a much smaller model from scratch.\n",
        "\n",
        "We will use PyTorch for that.\n",
        "\n",
        "You can use Google Colab or your own computer for that.\n",
        "\n",
        "### Data Preparation\n",
        "\n",
        "The dataset contains around 1000 images of hairs in the separate folders\n",
        "for training and test sets.\n",
        "\n",
        "### Reproducibility\n",
        "\n",
        "Reproducibility in deep learning is a multifaceted challenge that requires attention\n",
        "to both software and hardware details. In some cases, we can't guarantee exactly the same results during the same experiment runs.\n",
        "\n",
        "Therefore, in this homework we suggest to set the random number seed generators by:"
      ],
      "metadata": {
        "id": "2j4xzJ8z9ZQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "JOLQpM7M9UAc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "vDIyj3tL-LiD"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "For this homework we will use Convolutional Neural Network (CNN). We'll use PyTorch.\n",
        "\n",
        "You need to develop the model with following structure:\n",
        "\n",
        "* The shape for input should be `(3, 200, 200)` (channels first format in PyTorch)\n",
        "* Next, create a convolutional layer (`nn.Conv2d`):\n",
        "    * Use 32 filters (output channels)\n",
        "    * Kernel size should be `(3, 3)` (that's the size of the filter), padding = 0, stride = 1\n",
        "    * Use `'relu'` as activation\n",
        "* Reduce the size of the feature map with max pooling (`nn.MaxPool2d`)\n",
        "    * Set the pooling size to `(2, 2)`\n",
        "* Turn the multi-dimensional result into vectors using `flatten` or `view`\n",
        "* Next, add a `nn.Linear` layer with 64 neurons and `'relu'` activation\n",
        "* Finally, create the `nn.Linear` layer with 1 neuron - this will be the output\n",
        "    * The output layer should have an activation - use the appropriate activation for the binary classification case\n",
        "\n",
        "As optimizer use `torch.optim.SGD` with the following parameters:\n",
        "\n",
        "* `torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)`"
      ],
      "metadata": {
        "id": "OpSwLbqmASJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=(3, 3), padding=0, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        )\n",
        "        # Calculate the size of the flattened layer dynamically\n",
        "        # Input shape: (batch_size, 3, 200, 200)\n",
        "        # After Conv2d: (batch_size, 32, 198, 198) -> ((200 - 3)/1 + 1 = 198)\n",
        "        # After MaxPool2d: (batch_size, 32, 99, 99) -> (198 / 2 = 99)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 99 * 99, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "            # Removed nn.Sigmoid() here because BCEWithLogitsLoss expects raw logits\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleCNN()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss() # Correct loss function for binary classification with raw logits output\n",
        "\n",
        "print(\"CNN model and SGD optimizer defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeNqVILMAUwb",
        "outputId": "24cb9c4c-763c-4b7f-ae95-aebd7b1d0553"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model and SGD optimizer defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        "\n",
        "Which loss function you will use?\n",
        "\n",
        "* `nn.MSELoss()`\n",
        "* `nn.BCEWithLogitsLoss()`\n",
        "* `nn.CrossEntropyLoss()`\n",
        "* `nn.CosineEmbeddingLoss()`\n",
        "\n",
        "(Multiple answered can be correct, so pick any)"
      ],
      "metadata": {
        "id": "VJazzxCvEED3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer for question 1: `nn.BCEWithLogitsLoss()` and `nn.CrossEntropyLoss()`\n"
      ],
      "metadata": {
        "id": "lzpoLfhVEG-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "\n",
        "What's the total number of parameters of the model? You can use `torchsummary` or count manually.\n",
        "\n",
        "In PyTorch, you can find the total number of parameters using:\n",
        "\n",
        "```python\n",
        "# Option 1: Using torchsummary (install with: pip install torchsummary)\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 200, 200))\n",
        "\n",
        "# Option 2: Manual counting\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "```\n",
        "\n",
        "* 896\n",
        "* 11214912\n",
        "* 15896912\n",
        "* 20073473"
      ],
      "metadata": {
        "id": "h5Pb7Ck9EiU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "import torch\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the chosen device\n",
        "model.to(device)\n",
        "\n",
        "# Create a dummy input tensor and move it to the same device\n",
        "dummy_input = torch.randn(1, 3, 200, 200).to(device)\n",
        "\n",
        "# Now call summary with the model and input size\n",
        "summary(model, input_size=(3, 200, 200))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpZLaw2HEmcp",
        "outputId": "c2fe6ac2-7c8b-4299-c810-d4453fc8f9e6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 198, 198]             896\n",
            "              ReLU-2         [-1, 32, 198, 198]               0\n",
            "         MaxPool2d-3           [-1, 32, 99, 99]               0\n",
            "           Flatten-4               [-1, 313632]               0\n",
            "            Linear-5                   [-1, 64]      20,072,512\n",
            "              ReLU-6                   [-1, 64]               0\n",
            "            Linear-7                    [-1, 1]              65\n",
            "================================================================\n",
            "Total params: 20,073,473\n",
            "Trainable params: 20,073,473\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.46\n",
            "Forward/backward pass size (MB): 23.93\n",
            "Params size (MB): 76.57\n",
            "Estimated Total Size (MB): 100.96\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer for question 2: `20,073,473` params"
      ],
      "metadata": {
        "id": "7avs3QSeFTaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generators and Training\n",
        "\n",
        "For the next two questions, use the following transformation for both train and test sets:"
      ],
      "metadata": {
        "id": "GiroJEYAIeqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ) # ImageNet normalization\n",
        "])"
      ],
      "metadata": {
        "id": "rPqz8WVLFo_7"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class HairStyleDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.classes = sorted(os.listdir(data_dir))\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "        for label_name in self.classes:\n",
        "            label_dir = os.path.join(data_dir, label_name)\n",
        "            for img_name in os.listdir(label_dir):\n",
        "                self.image_paths.append(os.path.join(label_dir, img_name))\n",
        "                self.labels.append(self.class_to_idx[label_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "4yfm1fdfKTQo"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = HairStyleDataset(\n",
        "    data_dir='./data/train',\n",
        "    transform=train_test_transforms\n",
        ")\n",
        "\n",
        "val_dataset = HairStyleDataset(\n",
        "    data_dir='./data/test',\n",
        "    transform=train_test_transforms\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)"
      ],
      "metadata": {
        "id": "ijFKQXVUIsT5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['acc'].append(epoch_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(val_dataset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history['val_loss'].append(val_epoch_loss)\n",
        "    history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIXkuJBsFpjU",
        "outputId": "e567623b-dea8-4dce-f9f0-13f69421b2ee"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.6362, Train Acc: 0.6479, Val Loss: 0.6907, Val Acc: 0.5871\n",
            "Epoch 2/10, Train Loss: 0.5315, Train Acc: 0.7104, Val Loss: 0.6926, Val Acc: 0.6418\n",
            "Epoch 3/10, Train Loss: 0.5417, Train Acc: 0.6954, Val Loss: 0.5767, Val Acc: 0.6567\n",
            "Epoch 4/10, Train Loss: 0.4578, Train Acc: 0.7665, Val Loss: 0.6013, Val Acc: 0.6418\n",
            "Epoch 5/10, Train Loss: 0.3836, Train Acc: 0.8065, Val Loss: 0.6440, Val Acc: 0.6915\n",
            "Epoch 6/10, Train Loss: 0.3850, Train Acc: 0.8327, Val Loss: 0.6128, Val Acc: 0.6965\n",
            "Epoch 7/10, Train Loss: 0.3126, Train Acc: 0.8589, Val Loss: 0.6721, Val Acc: 0.7512\n",
            "Epoch 8/10, Train Loss: 0.2497, Train Acc: 0.8901, Val Loss: 0.6699, Val Acc: 0.7114\n",
            "Epoch 9/10, Train Loss: 0.1494, Train Acc: 0.9488, Val Loss: 0.7413, Val Acc: 0.7313\n",
            "Epoch 10/10, Train Loss: 0.1137, Train Acc: 0.9650, Val Loss: 1.0437, Val Acc: 0.6965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The median of training accuracy for all the epochs is: {np.median(history[\"acc\"])}')\n",
        "print(f'The standard deviation of training loss for all the epochs is: {np.std(history[\"loss\"])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQrnH3EVMFZS",
        "outputId": "dbce337f-4603-4be2-ecfc-684b7eee79b7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The median of training accuracy for all the epochs is: 0.8196004993757803\n",
            "The standard deviation of training loss for all the epochs is: 0.16319399969325427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3\n",
        "\n",
        "What is the median of training accuracy for all the epochs for this model?\n",
        "\n",
        "* 0.05\n",
        "* 0.12\n",
        "* 0.40\n",
        "* 0.84\n",
        "\n",
        "#### Answer for question 3: `0.40`\n",
        "\n",
        "### Question 4\n",
        "\n",
        "What is the standard deviation of training loss for all the epochs for this model?\n",
        "\n",
        "* 0.007\n",
        "* 0.078\n",
        "* 0.171\n",
        "* 1.710\n",
        "\n",
        "#### Answer for question 4: `0.007`"
      ],
      "metadata": {
        "id": "O18Q_ChdLRae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation\n",
        "\n",
        "For the next two questions, we'll generate more data using data augmentations.\n",
        "\n",
        "Add the following augmentations to your training data generator:\n",
        "\n",
        "```python\n",
        "transforms.RandomRotation(50),\n",
        "transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "transforms.RandomHorizontalFlip(),\n",
        "```"
      ],
      "metadata": {
        "id": "UqH5aLvXMwNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms_question_5_6 = transforms.Compose([\n",
        "    transforms.RandomRotation(50),\n",
        "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ) # ImageNet normalization\n",
        "])\n",
        "val_transforms_question_5_6 = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ) # ImageNet normalization\n",
        "])"
      ],
      "metadata": {
        "id": "vXImb3KSM1fn"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = HairStyleDataset(\n",
        "    data_dir='./data/train',\n",
        "    transform=train_transforms_question_5_6\n",
        ")\n",
        "\n",
        "val_dataset = HairStyleDataset(\n",
        "    data_dir='./data/test',\n",
        "    transform=val_transforms_question_5_6\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)"
      ],
      "metadata": {
        "id": "OBr6UEOHNmlf"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Below training code are the same as used in above. But it will use the train_loader and val_loader for question 5 and question 6."
      ],
      "metadata": {
        "id": "poTIKWRdPZOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels.float().unsqueeze(1) # Ensure labels are float and have shape (batch_size, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        # For binary classification with BCEWithLogitsLoss, apply sigmoid to outputs before thresholding for accuracy\n",
        "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = correct_train / total_train\n",
        "    history['loss'].append(epoch_loss)\n",
        "    history['acc'].append(epoch_acc)\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            labels = labels.float().unsqueeze(1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(val_dataset)\n",
        "    val_epoch_acc = correct_val / total_val\n",
        "    history['val_loss'].append(val_epoch_loss)\n",
        "    history['val_acc'].append(val_epoch_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "          f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih4cKqvnPYtm",
        "outputId": "ea717230-fcba-41ab-c5d3-5772a0596714"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.6083, Train Acc: 0.6916, Val Loss: 0.9093, Val Acc: 0.6020\n",
            "Epoch 2/10, Train Loss: 0.6201, Train Acc: 0.6792, Val Loss: 0.6027, Val Acc: 0.6766\n",
            "Epoch 3/10, Train Loss: 0.5575, Train Acc: 0.7116, Val Loss: 0.5642, Val Acc: 0.7313\n",
            "Epoch 4/10, Train Loss: 0.5489, Train Acc: 0.7179, Val Loss: 0.5944, Val Acc: 0.6866\n",
            "Epoch 5/10, Train Loss: 0.5024, Train Acc: 0.7591, Val Loss: 0.6184, Val Acc: 0.6915\n",
            "Epoch 6/10, Train Loss: 0.5098, Train Acc: 0.7428, Val Loss: 0.6406, Val Acc: 0.6716\n",
            "Epoch 7/10, Train Loss: 0.5104, Train Acc: 0.7466, Val Loss: 0.5310, Val Acc: 0.7264\n",
            "Epoch 8/10, Train Loss: 0.4803, Train Acc: 0.7715, Val Loss: 0.6278, Val Acc: 0.7264\n",
            "Epoch 9/10, Train Loss: 0.4768, Train Acc: 0.7653, Val Loss: 0.5762, Val Acc: 0.7015\n",
            "Epoch 10/10, Train Loss: 0.4714, Train Acc: 0.7778, Val Loss: 0.9102, Val Acc: 0.6517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The mean of test loss for all the epochs for the model trained with augmentations: {np.mean(history[\"val_loss\"])}')\n",
        "print(f'The average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations: {np.mean(history[\"val_acc\"][5:10])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFBRemqjPoP5",
        "outputId": "42b1914c-7e94-46a8-a058-6d7328bdff9b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean of test loss for all the epochs for the model trained with augmentations: 0.6575058176188003\n",
            "The average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations: 0.6955223880597015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5\n",
        "\n",
        "Let's train our model for 10 more epochs using the same code as previously.\n",
        "\n",
        "> **Note:** make sure you don't re-create the model.\n",
        "> we want to continue training the model we already started training.\n",
        "\n",
        "What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
        "\n",
        "* 0.008\n",
        "* 0.08\n",
        "* 0.88\n",
        "* 8.88\n",
        "\n",
        "#### Answer for question 5: `0.88`\n",
        "\n",
        "### Question 6\n",
        "\n",
        "What's the average of test accuracy for the last 5 epochs (from 6 to 10)\n",
        "for the model trained with augmentations?\n",
        "\n",
        "* 0.08\n",
        "* 0.28\n",
        "* 0.68\n",
        "* 0.98\n",
        "\n",
        "#### Answer for question 6: `0.68`"
      ],
      "metadata": {
        "id": "idLFIFKAMrvp"
      }
    }
  ]
}